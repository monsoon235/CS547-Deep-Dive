{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T02:48:59.810481Z",
     "start_time": "2024-11-15T02:48:57.629058Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7a2acbb690>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "import tqdm\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe54d694cbde6aee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:04:47.690050Z",
     "start_time": "2024-11-15T03:04:47.605910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_encoded: Index(['Year_2010', 'Year_2011', 'Year_2012', 'Year_2013', 'Year_2014',\n",
      "       'Year_2015', 'Year_2016', 'Year_2018', 'Year_2019', 'Year_2020',\n",
      "       'Year_2021', 'Year_2022', 'Year_2023', 'Year_2024', 'Term_0', 'Term_1',\n",
      "       'Term_2', 'Term_3'],\n",
      "      dtype='object') (58915, 18)\n",
      "data: (58915, 24)\n",
      "# groups: 4048\n",
      "train: torch.Size([4048, 334, 18]) torch.Size([4048]) torch.Size([4048, 334, 1]) torch.Size([4048])\n",
      "test: torch.Size([4048, 335, 18]) torch.Size([4048]) torch.Size([4048, 335, 1]) torch.Size([4048])\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_pickle('data/full.pkl')\n",
    "\n",
    "# filter out useless columns\n",
    "data = data.drop(columns=['W', 'YearTerm'])\n",
    "\n",
    "# filter out missing fields\n",
    "data = data.dropna()\n",
    "\n",
    "# compute GPA of each course\n",
    "data['Student Number'] = data['A+'] + data['A'] + data['A-'] + data['B+'] + data['B'] + data['B-'] + data['C+'] + data['C'] + data['C-'] + data['D+'] + data['D'] + data['D-'] + data['F']\n",
    "grade_mapping = {\n",
    "    'A+': 4.0,\n",
    "    'A': 4.0,\n",
    "    'A-': 3.7,\n",
    "    'B+': 3.3,\n",
    "    'B': 3.0,\n",
    "    'B-': 2.7,\n",
    "    'C+': 2.3,\n",
    "    'C': 2.0,\n",
    "    'C-': 1.7,\n",
    "    'D+': 1.3,\n",
    "    'D': 1.0,\n",
    "    'D-': 0.7,\n",
    "    'F': 0.0\n",
    "}\n",
    "data['GPA'] = 0\n",
    "for col in ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D', 'D-', 'F']:\n",
    "    data['GPA'] += grade_mapping[col] * data[col]\n",
    "data['GPA'] /= data['Student Number']\n",
    "\n",
    "# convert student number into percentage\n",
    "for col in ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D', 'D-', 'F']:\n",
    "    data[col] = data[col] / data['Student Number']\n",
    "\n",
    "group_columns = ['Number', 'Course Title', 'Subject', 'Primary Instructor', 'Sched Type']\n",
    "feature_columns = ['Year', 'Term']\n",
    "# label_columns = ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D', 'D-', 'F', 'GPA']\n",
    "label_columns = ['GPA']\n",
    "\n",
    "# convert term into integer\n",
    "data[\"Term\"] = data[\"Term\"].map({\"Winter\": 0, \"Spring\": 1, \"Summer\": 2, \"Fall\": 3})\n",
    "\n",
    "# data sort\n",
    "data = data.sort_values(by=['Year', 'Term'], ascending=True)\n",
    "\n",
    "# encoding feature columns: one-hot encoding\n",
    "feature_encoded = pd.get_dummies(data[feature_columns].astype(str))\n",
    "feature_columns = feature_encoded.columns\n",
    "print(\"feature_encoded:\", feature_encoded.columns, feature_encoded.shape)\n",
    "data = pd.concat([data[group_columns], feature_encoded, data[label_columns]], axis=1)\n",
    "print(\"data:\", data.shape)\n",
    "\n",
    "data\n",
    "\n",
    "# split dataset by Number, Course Title, and Primary Instructor into groups\n",
    "groups = data.groupby(group_columns)\n",
    "group_dict = {}\n",
    "for (number, title, subject, instructor, sched_type), group in groups:\n",
    "    if group.shape[0] < 4: # filter series with less than 4 records\n",
    "        continue\n",
    "    # print(number, title, subject, instructor, sched_type, group.shape)\n",
    "    group_dict[(number, title, subject, instructor, sched_type)] = group\n",
    "print(\"# groups:\", len(group_dict))\n",
    "\n",
    "# generate data for NN\n",
    "course_ids = []\n",
    "seqs = []\n",
    "lens = []\n",
    "tgts = []\n",
    "for (key, group), i in zip(group_dict.items(), range(len(group_dict))):\n",
    "    course_ids.append(i)\n",
    "    seqs.append(group[feature_columns].to_numpy())\n",
    "    lens.append(group.shape[0])\n",
    "    tgts.append(group[label_columns].to_numpy())\n",
    "\n",
    "# split train/test\n",
    "## each seq: [:-1] for train, and all for test\n",
    "seqs_train = []\n",
    "lens_train = []\n",
    "tgts_train = []\n",
    "seqs_test = []\n",
    "lens_test = []\n",
    "tgts_test = []\n",
    "for seq, l, tgt in zip(seqs, lens, tgts):\n",
    "    seqs_train.append(seq[:-1])\n",
    "    lens_train.append(l-1)\n",
    "    tgts_train.append(tgt[:-1])\n",
    "    seqs_test.append(seq)\n",
    "    lens_test.append(l)\n",
    "    tgts_test.append(tgt)\n",
    "\n",
    "# to torch tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "course_ids = torch.tensor(course_ids).to(torch.int64)\n",
    "seqs_train = pad_sequence([torch.tensor(seq) for seq in seqs_train], batch_first=True).to(torch.float32)\n",
    "lens_train = torch.tensor(lens_train).to(torch.float32)\n",
    "tgts_train = pad_sequence([torch.tensor(tgt) for tgt in tgts_train], batch_first=True).to(torch.float32)\n",
    "seqs_test = pad_sequence([torch.tensor(seq) for seq in seqs_test], batch_first=True).to(torch.float32)\n",
    "lens_test = torch.tensor(lens_test).to(torch.float32)\n",
    "tgts_test = pad_sequence([torch.tensor(tgt) for tgt in tgts_test], batch_first=True).to(torch.float32)\n",
    "print(\"train:\", seqs_train.shape, lens_train.shape, tgts_train.shape, course_ids.shape)\n",
    "print(\"test:\", seqs_test.shape, lens_test.shape, tgts_test.shape, course_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4be4db8f788182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T02:48:59.981068Z",
     "start_time": "2024-11-15T02:48:59.975769Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# training functions\n",
    "\n",
    "class GPAPredictRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, course_vocab_size, embedding_dim, hidden_size, num_layers, bias, dropout):\n",
    "        super(GPAPredictRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(course_vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=input_size + embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bias = bias,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, seq, course_id):\n",
    "        embedding = self.embedding(course_id)\n",
    "        embedding = torch.stack([embedding] * seq.shape[1], dim=1)\n",
    "        input_ = torch.concat([seq, embedding], dim=-1)\n",
    "        out, _ = self.rnn(input_)\n",
    "        out = self.fc(out)\n",
    "        gpa = 4.0 * self.tanh(out)\n",
    "        return gpa\n",
    "\n",
    "class GPADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, seqs, tgts, lens, course_ids, device, in_test=False):\n",
    "        self.seqs = seqs\n",
    "        self.tgts = tgts\n",
    "        self.lens = lens\n",
    "        self.course_ids = course_ids\n",
    "        self.device = device\n",
    "        self.in_test = in_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.seqs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seqs[idx].to(self.device)\n",
    "        tgt = self.tgts[idx].to(self.device)\n",
    "        course_id = self.course_ids[idx].to(self.device)\n",
    "        len_ = int(self.lens[idx])\n",
    "        mask = torch.zeros_like(seq[..., 0])\n",
    "        if self.in_test:\n",
    "            mask[len_ - 1] = 1\n",
    "        else:\n",
    "            mask[:len_] = 1\n",
    "        return seq, tgt, mask, course_id\n",
    "\n",
    "def loss_fn(tgt_pred, tgt_gt, mask):\n",
    "    tgt_pred = tgt_pred * mask[..., None]\n",
    "    tgt_gt = tgt_gt * mask[..., None]\n",
    "    loss = (tgt_pred[..., -1] - tgt_gt[..., -1]) ** 2  # only for GPA\n",
    "    loss = loss.sum() / mask.sum()\n",
    "    return loss\n",
    "\n",
    "def get_model(input_size, output_size, course_vocab_size, embedding_dim, hidden_size, num_layers, bias, dropout):\n",
    "    model = GPAPredictRNN(\n",
    "        input_size=input_size,\n",
    "        output_size=output_size,\n",
    "        course_vocab_size=course_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        bias=bias,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74dab021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ckpt\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_config = {\n",
    "    \"input_size\": seqs_train.shape[-1],\n",
    "    \"output_size\": tgts_train.shape[-1],\n",
    "    \"course_vocab_size\": len(course_ids),\n",
    "    \"embedding_dim\": 64,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_layers\": 10,\n",
    "    \"bias\": True,\n",
    "    \"dropout\": 0.2,\n",
    "}\n",
    "model = get_model(**model_config)\n",
    "model.load_state_dict(torch.load('models/model-embedding_dim=64,hidden_size=64,num_layers=10,bias=True,dropout=0.2,optimizer=Adam,learning_rate=1e-05,batch_size=32.pt', map_location='cpu', weights_only=True))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_baseline: 0.050872107269242406\n",
      "permuted feature dim 0 (Year_2010) 0.05103168191853911\n",
      "permuted feature dim 1 (Year_2011) 0.05092426899354905\n",
      "permuted feature dim 2 (Year_2012) 0.05105029370170087\n",
      "permuted feature dim 3 (Year_2013) 0.050909606460481885\n",
      "permuted feature dim 4 (Year_2014) 0.050954789575189355\n",
      "permuted feature dim 5 (Year_2015) 0.05087237821426242\n",
      "permuted feature dim 6 (Year_2016) 0.05101152113638818\n",
      "permuted feature dim 7 (Year_2018) 0.050962435896508396\n",
      "permuted feature dim 8 (Year_2019) 0.052149775857105854\n",
      "permuted feature dim 9 (Year_2020) 0.05149591697845608\n",
      "permuted feature dim 10 (Year_2021) 0.05224884555209428\n",
      "permuted feature dim 11 (Year_2022) 0.05247324642259628\n",
      "permuted feature dim 12 (Year_2023) 0.052360879234038295\n",
      "permuted feature dim 13 (Year_2024) 0.050871631945483387\n",
      "permuted feature dim 14 (Term_0) 0.05089383579324931\n",
      "permuted feature dim 15 (Term_1) 0.05105609730817377\n",
      "permuted feature dim 16 (Term_2) 0.05091075110249221\n",
      "permuted feature dim 17 (Term_3) 0.05134212719276547\n",
      "permuted course_id: 0.26386514827609064\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(model, dataloader):\n",
    "    loss_total = 0\n",
    "    num_seqs = 0\n",
    "    with torch.no_grad():\n",
    "        for seqs_batch, tgts_batch, mask_batch, course_id_batch in dataloader:\n",
    "            tgts_pred = model(seqs_batch, course_id_batch)\n",
    "            loss = loss_fn(tgts_pred, tgts_batch, mask_batch)\n",
    "            num_seqs += seqs_batch.shape[0]\n",
    "            loss_total += loss.item() * seqs_batch.shape[0]\n",
    "    val_loss = loss_total / num_seqs\n",
    "    return val_loss\n",
    "\n",
    "def shuffle_tensor(tensor, dim):\n",
    "    idx = torch.randperm(tensor.shape[dim], device=tensor.device)\n",
    "    tensor = torch.index_select(tensor, dim, idx)\n",
    "    return tensor\n",
    "\n",
    "losses = []\n",
    "num_permutation = 10\n",
    "\n",
    "seqs_test = seqs_test.to(device=device)\n",
    "course_ids = course_ids.to(device=device)\n",
    "tgts_test = tgts_test.to(device=device)\n",
    "masks = torch.ones_like(seqs_test[..., 0], device=device)\n",
    "for i in range(seqs_test.shape[0]):\n",
    "    masks[i, int(lens_test[i]) - 1] = 1\n",
    "\n",
    "# feature importance\n",
    "\n",
    "## baseline\n",
    "dataset = GPADataset(seqs_test, tgts_test, lens_test, course_ids, device, in_test = True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=512)\n",
    "loss_baseline = compute_loss(model, dataloader)\n",
    "\n",
    "## input feature dimension: 18+1\n",
    "for dim in range(seqs_test.shape[-1] + 1):\n",
    "    # print(\"dim\", dim)\n",
    "    loss = 0\n",
    "    for _ in range(num_permutation):\n",
    "        if dim == seqs_test.shape[-1]:\n",
    "            seqs_input = seqs_test\n",
    "            course_ids_input = shuffle_tensor(course_ids, 0)\n",
    "        else:\n",
    "            seqs_input = seqs_test.clone()\n",
    "            seqs_input[..., dim] = shuffle_tensor(seqs_test[..., dim], 0)\n",
    "            course_ids_input = course_ids\n",
    "        dataset = GPADataset(seqs_input, tgts_test, lens_test, course_ids_input, device, in_test = True)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=512)\n",
    "        loss += compute_loss(model, dataloader)\n",
    "    losses.append(float(loss) / num_permutation)\n",
    "\n",
    "# print feature importance\n",
    "print(\"loss_baseline:\", loss_baseline)\n",
    "for i, loss in enumerate(losses):\n",
    "    if i == seqs_test.shape[-1]:\n",
    "        print(\"permuted course_id:\", loss)\n",
    "    else:\n",
    "        feature_name = feature_columns[i]\n",
    "        print(f\"permuted feature dim {i} ({feature_name})\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04532d3b",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The most important feature is the course id, and other features have much less importance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs547",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
